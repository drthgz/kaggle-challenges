{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da648930",
   "metadata": {},
   "source": [
    "# Heart Disease Prediction - Binary Classification\n",
    "## Playground Series S6E2 - Optimized Pipeline\n",
    "\n",
    "**Goal**: Predict heart disease probability (AUC-ROC metric)\n",
    "**Approach**: Feature engineering + Stacking + Blending\n",
    "**Target**: Top 10 (~24 position)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befc8475",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d92a1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All imports successful\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, classification_report\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ“ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04b3d3b",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d493b5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally\n",
      "Data path: data\n",
      "Output path: submissions\n"
     ]
    }
   ],
   "source": [
    "# Check if running locally or on Kaggle\n",
    "import os\n",
    "IS_KAGGLE = os.path.exists('/kaggle')\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    DATA_PATH = Path('/kaggle/input/playground-series-s6e2')\n",
    "    OUTPUT_PATH = Path('/kaggle/working')\n",
    "    print(\"Running on Kaggle\")\n",
    "else:\n",
    "    DATA_PATH = Path('./data')\n",
    "    OUTPUT_PATH = Path('./submissions')\n",
    "    print(\"Running locally\")\n",
    "\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"Output path: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b37b0ef",
   "metadata": {},
   "source": [
    "## 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "517c12d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (630000, 15)\n",
      "Test shape: (270000, 14)\n",
      "\n",
      "Train columns: ['id', 'Age', 'Sex', 'Chest pain type', 'BP', 'Cholesterol', 'FBS over 120', 'EKG results', 'Max HR', 'Exercise angina', 'ST depression', 'Slope of ST', 'Number of vessels fluro', 'Thallium', 'Heart Disease']\n",
      "\n",
      "First rows of train:\n",
      "   id  Age  Sex  Chest pain type   BP  Cholesterol  FBS over 120  EKG results  \\\n",
      "0   0   58    1                4  152          239             0            0   \n",
      "1   1   52    1                1  125          325             0            2   \n",
      "2   2   56    0                2  160          188             0            2   \n",
      "3   3   44    0                3  134          229             0            2   \n",
      "4   4   58    1                4  140          234             0            2   \n",
      "\n",
      "   Max HR  Exercise angina  ST depression  Slope of ST  \\\n",
      "0     158                1            3.6            2   \n",
      "1     171                0            0.0            1   \n",
      "2     151                0            0.0            1   \n",
      "3     150                0            1.0            2   \n",
      "4     125                1            3.8            2   \n",
      "\n",
      "   Number of vessels fluro  Thallium Heart Disease  \n",
      "0                        2         7      Presence  \n",
      "1                        0         3       Absence  \n",
      "2                        0         3       Absence  \n",
      "3                        0         3       Absence  \n",
      "4                        3         3      Presence  \n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "train_df = pd.read_csv(DATA_PATH / 'train.csv')\n",
    "test_df = pd.read_csv(DATA_PATH / 'test.csv')\n",
    "submission_sample = pd.read_csv(DATA_PATH / 'sample_submission.csv')\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(f\"\\nTrain columns: {list(train_df.columns)}\")\n",
    "print(f\"\\nFirst rows of train:\")\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd79c4d",
   "metadata": {},
   "source": [
    "## 4. EDA & Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83748ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types and missing values\n",
    "print(\"=\"*60)\n",
    "print(\"DATA OVERVIEW\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nData types:\")\n",
    "print(train_df.dtypes)\n",
    "\n",
    "print(\"\\nMissing values:\")\n",
    "print(train_df.isnull().sum())\n",
    "\n",
    "# Feature analysis\n",
    "print(\"\\nTarget variable distribution:\")\n",
    "print(train_df['Heart Disease'].value_counts())\n",
    "print(f\"\\nTarget ratio: {train_df['Heart Disease'].mean():.4f} (% positive)\")\n",
    "\n",
    "# Identify feature types\n",
    "numeric_features = train_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = train_df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Remove id and target from features\n",
    "if 'id' in numeric_features:\n",
    "    numeric_features.remove('id')\n",
    "if 'Heart Disease' in numeric_features:\n",
    "    numeric_features.remove('Heart Disease')\n",
    "\n",
    "print(f\"\\nNumeric features ({len(numeric_features)}): {numeric_features[:5]}...\")\n",
    "print(f\"Categorical features ({len(categorical_features)}): {categorical_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a7d109",
   "metadata": {},
   "source": [
    "## 5. Feature Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc38efdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations with target\n",
    "correlations = train_df[numeric_features + ['Heart Disease']].corr()['Heart Disease'].drop('Heart Disease')\n",
    "correlations = correlations.abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"Feature Correlations with Target (absolute):\")\n",
    "print(correlations)\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NUMERIC FEATURE STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(train_df[numeric_features].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15b8107",
   "metadata": {},
   "source": [
    "## 6. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8354f76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, categorical_features, numeric_features, fit_scalers=True, scalers=None):\n",
    "    \"\"\"Encode categoricals, scale numerics\"\"\"\n",
    "    df_proc = df.copy()\n",
    "    \n",
    "    if fit_scalers:\n",
    "        scalers = {}\n",
    "        # Encode categorical features\n",
    "        for col in categorical_features:\n",
    "            if col in df_proc.columns:\n",
    "                le = LabelEncoder()\n",
    "                df_proc[col] = le.fit_transform(df_proc[col].astype(str))\n",
    "                scalers[col] = le\n",
    "        \n",
    "        # Scale numeric features\n",
    "        if numeric_features:\n",
    "            scaler = StandardScaler()\n",
    "            df_proc[numeric_features] = scaler.fit_transform(df_proc[numeric_features])\n",
    "            scalers['numeric'] = scaler\n",
    "    else:\n",
    "        # Transform using fitted scalers\n",
    "        for col in categorical_features:\n",
    "            if col in df_proc.columns and col in scalers:\n",
    "                df_proc[col] = scalers[col].transform(df_proc[col].astype(str))\n",
    "        \n",
    "        if numeric_features and 'numeric' in scalers:\n",
    "            df_proc[numeric_features] = scalers['numeric'].transform(df_proc[numeric_features])\n",
    "    \n",
    "    return df_proc, scalers\n",
    "\n",
    "# Preprocess training data\n",
    "train_processed, scalers = preprocess_data(train_df.drop('id', axis=1), \n",
    "                                           categorical_features, \n",
    "                                           numeric_features,\n",
    "                                           fit_scalers=True)\n",
    "\n",
    "# Preprocess test data\n",
    "test_processed, _ = preprocess_data(test_df.drop('id', axis=1), \n",
    "                                    categorical_features, \n",
    "                                    numeric_features,\n",
    "                                    fit_scalers=False, \n",
    "                                    scalers=scalers)\n",
    "\n",
    "print(\"âœ“ Preprocessing complete\")\n",
    "print(f\"Train shape: {train_processed.shape}\")\n",
    "print(f\"Test shape: {test_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ccf15b",
   "metadata": {},
   "source": [
    "## 7. Train/Val Split & Feature Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906810e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X_all = train_processed.drop('Heart Disease', axis=1)\n",
    "y_all = train_processed['Heart Disease']\n",
    "X_test = test_processed.drop('Heart Disease', axis=1) if 'Heart Disease' in test_processed.columns else test_processed\n",
    "\n",
    "# 80/20 split for baseline\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_all, y_all, test_size=0.2, \n",
    "                                                    random_state=42, stratify=y_all)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"\\nTarget ratio - Train: {y_train.mean():.4f}, Val: {y_val.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e8a68a",
   "metadata": {},
   "source": [
    "## 8. Baseline Model - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd881df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"BASELINE MODEL - LOGISTIC REGRESSION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "baseline_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = baseline_model.predict_proba(X_train)[:, 1]\n",
    "y_val_pred = baseline_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "train_auc = roc_auc_score(y_train, y_train_pred)\n",
    "val_auc = roc_auc_score(y_val, y_val_pred)\n",
    "\n",
    "print(f\"Training AUC-ROC: {train_auc:.4f}\")\n",
    "print(f\"Validation AUC-ROC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa592fa",
   "metadata": {},
   "source": [
    "## 9. Model Comparison - 5-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf23e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_score(y_true, y_pred):\n",
    "    \"\"\"Calculate AUC-ROC\"\"\"\n",
    "    return roc_auc_score(y_true, y_pred)\n",
    "\n",
    "def cross_validate_classifier(model_builder, X, y, name, folds=5):\n",
    "    \"\"\"5-fold cross-validation for classifiers\"\"\"\n",
    "    kf = KFold(n_splits=folds, shuffle=False, random_state=42)\n",
    "    scores = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "        model = model_builder()\n",
    "        model.fit(X.iloc[train_idx], y.iloc[train_idx])\n",
    "        val_preds = model.predict_proba(X.iloc[val_idx])[:, 1]\n",
    "        score = auc_score(y.iloc[val_idx], val_preds)\n",
    "        scores.append(score)\n",
    "    \n",
    "    mean_score = np.mean(scores)\n",
    "    std_score = np.std(scores)\n",
    "    print(f\"{name:15s}: {mean_score:.4f} Â± {std_score:.4f}\")\n",
    "    return mean_score, std_score\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL COMPARISON - 5-FOLD CROSS-VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Model builders\n",
    "def build_lgb():\n",
    "    return LGBMClassifier(n_estimators=200, learning_rate=0.08, num_leaves=80, \n",
    "                         max_depth=8, subsample=0.8, colsample_bytree=0.8,\n",
    "                         reg_alpha=0.1, reg_lambda=0.3, random_state=42, verbose=-1)\n",
    "\n",
    "def build_xgb():\n",
    "    return XGBClassifier(n_estimators=250, learning_rate=0.05, max_depth=5,\n",
    "                        subsample=0.8, colsample_bytree=0.8, reg_alpha=0.1,\n",
    "                        reg_lambda=0.5, random_state=42, verbosity=0)\n",
    "\n",
    "def build_cat():\n",
    "    return CatBoostClassifier(iterations=200, learning_rate=0.08, depth=7,\n",
    "                             subsample=0.8, random_state=42, verbose=0)\n",
    "\n",
    "# Test each model\n",
    "lgb_mean, lgb_std = cross_validate_classifier(build_lgb, X_all, y_all, \"LightGBM\")\n",
    "xgb_mean, xgb_std = cross_validate_classifier(build_xgb, X_all, y_all, \"XGBoost\")\n",
    "cat_mean, cat_std = cross_validate_classifier(build_cat, X_all, y_all, \"CatBoost\")\n",
    "\n",
    "print(f\"\\nBest model: {'LightGBM' if lgb_mean > xgb_mean and lgb_mean > cat_mean else 'XGBoost' if xgb_mean > cat_mean else 'CatBoost'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6450e631",
   "metadata": {},
   "source": [
    "## 10. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2516f1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get original numeric features from unprocessed data\n",
    "X_train_orig = train_df.loc[X_train.index].drop(['id', 'Heart Disease'], axis=1)\n",
    "X_val_orig = train_df.loc[X_val.index].drop(['id', 'Heart Disease'], axis=1)\n",
    "X_test_orig = test_df.drop('id', axis=1)\n",
    "\n",
    "def create_features(df_orig, df_proc):\n",
    "    \"\"\"Create engineered features for classification\"\"\"\n",
    "    df_eng = df_proc.copy()\n",
    "    \n",
    "    # Get numeric columns from original (unscaled) data\n",
    "    numeric_cols = [col for col in numeric_features if col in df_orig.columns]\n",
    "    \n",
    "    if len(numeric_cols) > 0:\n",
    "        # Interactions between top correlated features\n",
    "        if len(numeric_cols) >= 2:\n",
    "            # Pairwise interactions\n",
    "            for i in range(min(3, len(numeric_cols))):\n",
    "                for j in range(i+1, min(4, len(numeric_cols))):\n",
    "                    col1, col2 = numeric_cols[i], numeric_cols[j]\n",
    "                    df_eng[f'{col1}_x_{col2}'] = df_orig[col1] * df_orig[col2]\n",
    "        \n",
    "        # Polynomial features for top features\n",
    "        for col in numeric_cols[:3]:\n",
    "            df_eng[f'{col}_sq'] = df_orig[col] ** 2\n",
    "            df_eng[f'{col}_sqrt'] = np.sqrt(np.abs(df_orig[col]))\n",
    "        \n",
    "        # Ratio features\n",
    "        if len(numeric_cols) >= 2:\n",
    "            df_eng[f'{numeric_cols[0]}_div_{numeric_cols[1]}'] = (df_orig[numeric_cols[0]] / \n",
    "                                                                    (np.abs(df_orig[numeric_cols[1]]) + 1e-5))\n",
    "    \n",
    "    return df_eng\n",
    "\n",
    "# Create engineered features\n",
    "X_train_eng = create_features(X_train_orig, X_train)\n",
    "X_val_eng = create_features(X_val_orig, X_val)\n",
    "X_test_eng = create_features(X_test_orig, X_test)\n",
    "\n",
    "print(f\"Original features: {X_train.shape[1]}\")\n",
    "print(f\"Engineered features: {X_train_eng.shape[1]}\")\n",
    "print(f\"New features added: {X_train_eng.shape[1] - X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6326d8",
   "metadata": {},
   "source": [
    "## 11. Feature Set Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257d9bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"FEATURE SET COMPARISON - 3-FOLD CV\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def quick_cv(X, y, model_builder, folds=3):\n",
    "    \"\"\"Quick 3-fold CV for comparison\"\"\"\n",
    "    kf = KFold(n_splits=folds, shuffle=False, random_state=42)\n",
    "    scores = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "        model = model_builder()\n",
    "        model.fit(X.iloc[train_idx], y.iloc[train_idx])\n",
    "        val_preds = model.predict_proba(X.iloc[val_idx])[:, 1]\n",
    "        score = auc_score(y.iloc[val_idx], val_preds)\n",
    "        scores.append(score)\n",
    "    \n",
    "    return np.mean(scores), np.std(scores)\n",
    "\n",
    "# Compare feature sets\n",
    "orig_mean, orig_std = quick_cv(X_all, y_all, build_lgb, folds=3)\n",
    "eng_mean, eng_std = quick_cv(X_train_eng.append(X_val_eng), y_all, build_lgb, folds=3)\n",
    "\n",
    "print(f\"Original features:   {orig_mean:.4f} Â± {orig_std:.4f} AUC-ROC\")\n",
    "print(f\"Engineered features: {eng_mean:.4f} Â± {eng_std:.4f} AUC-ROC\")\n",
    "print(f\"Improvement: {(eng_mean - orig_mean):.4f} AUC-ROC\")\n",
    "\n",
    "# Use engineered features\n",
    "if eng_mean > orig_mean:\n",
    "    print(\"âœ“ Using engineered features\")\n",
    "else:\n",
    "    print(\"âœ“ Using original features\")\n",
    "    X_train_eng, X_val_eng, X_test_eng = X_train.copy(), X_val.copy(), X_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b5e897",
   "metadata": {},
   "source": [
    "## 12. Stacking Ensemble - 5-Fold Meta-Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae52d021",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"STACKING ENSEMBLE - 5-FOLD CV\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Combine train/val for full dataset stacking\n",
    "X_stack = pd.concat([X_train_eng, X_val_eng], axis=0)\n",
    "y_stack = pd.concat([y_train, y_val], axis=0)\n",
    "\n",
    "# 5-fold CV for meta-features\n",
    "kf_stack = KFold(n_splits=5, shuffle=False, random_state=42)\n",
    "meta_train = np.zeros((len(X_stack), 2))  # 2 base models\n",
    "meta_test = np.zeros((len(X_test_eng), 2))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf_stack.split(X_stack)):\n",
    "    print(f\"Fold {fold+1}/5...\", end=\" \")\n",
    "    \n",
    "    # Train base models on fold\n",
    "    lgb_base = build_lgb()\n",
    "    lgb_base.fit(X_stack.iloc[train_idx], y_stack.iloc[train_idx])\n",
    "    \n",
    "    xgb_base = build_xgb()\n",
    "    xgb_base.fit(X_stack.iloc[train_idx], y_stack.iloc[train_idx])\n",
    "    \n",
    "    # Meta-features on validation fold\n",
    "    meta_train[val_idx, 0] = lgb_base.predict_proba(X_stack.iloc[val_idx])[:, 1]\n",
    "    meta_train[val_idx, 1] = xgb_base.predict_proba(X_stack.iloc[val_idx])[:, 1]\n",
    "    \n",
    "    # Average test predictions\n",
    "    meta_test[:, 0] += lgb_base.predict_proba(X_test_eng)[:, 1] / 5\n",
    "    meta_test[:, 1] += xgb_base.predict_proba(X_test_eng)[:, 1] / 5\n",
    "    \n",
    "    lgb_val_auc = auc_score(y_stack.iloc[val_idx], meta_train[val_idx, 0])\n",
    "    xgb_val_auc = auc_score(y_stack.iloc[val_idx], meta_train[val_idx, 1])\n",
    "    print(f\"LGB={lgb_val_auc:.4f}, XGB={xgb_val_auc:.4f}\")\n",
    "\n",
    "# Train meta-learner (Ridge for classification)\n",
    "meta_learner = Ridge(alpha=1.0)\n",
    "meta_learner.fit(meta_train, y_stack)\n",
    "\n",
    "# Meta-learner predictions\n",
    "stack_train_preds = meta_learner.predict(meta_train)\n",
    "stack_test_preds = meta_learner.predict(meta_test)\n",
    "\n",
    "print(f\"\\nStack train AUC: {auc_score(y_stack, stack_train_preds):.4f}\")\n",
    "print(f\"Meta-learner weights: LGB={meta_learner.coef_[0]:.4f}, XGB={meta_learner.coef_[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feefce36",
   "metadata": {},
   "source": [
    "## 13. Blending - Final Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d3f184",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"BLENDING - WEIGHTED ENSEMBLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train final base models on full dataset\n",
    "lgb_final = build_lgb()\n",
    "lgb_final.fit(X_stack, y_stack)\n",
    "lgb_preds_test = lgb_final.predict_proba(X_test_eng)[:, 1]\n",
    "\n",
    "xgb_final = build_xgb()\n",
    "xgb_final.fit(X_stack, y_stack)\n",
    "xgb_preds_test = xgb_final.predict_proba(X_test_eng)[:, 1]\n",
    "\n",
    "# Blend: weight 0.4 LGB, 0.4 XGB, 0.2 Stacking\n",
    "blend_preds = 0.4 * lgb_preds_test + 0.4 * xgb_preds_test + 0.2 * stack_test_preds\n",
    "\n",
    "print(f\"\\nâœ“ Blend weights: LGB=0.4, XGB=0.4, Stacking=0.2\")\n",
    "print(f\"Blend predictions shape: {blend_preds.shape}\")\n",
    "print(f\"Blend prediction range: [{blend_preds.min():.4f}, {blend_preds.max():.4f}]\")\n",
    "print(f\"Mean: {blend_preds.mean():.4f}, Std: {blend_preds.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94577bac",
   "metadata": {},
   "source": [
    "## 14. Final Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e907b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"FINAL SUBMISSION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create submission\n",
    "submission_final = submission_sample.copy()\n",
    "submission_final['Heart Disease'] = blend_preds\n",
    "submission_final.to_csv(OUTPUT_PATH / 'submission.csv', index=False)\n",
    "\n",
    "print(f\"\\nâœ“ Submission saved to submission.csv\")\n",
    "print(f\"  Format: Stacking (LGB + XGB) + Blending\")\n",
    "print(f\"  Features: {X_test_eng.shape[1]} total\")\n",
    "print(f\"\\nSubmission preview:\")\n",
    "print(submission_final.head(10))\n",
    "print(f\"\\nSubmission stats:\")\n",
    "print(f\"  Records: {len(submission_final)}\")\n",
    "print(f\"  Mean: {blend_preds.mean():.4f}\")\n",
    "print(f\"  Min:  {blend_preds.min():.4f}\")\n",
    "print(f\"  Max:  {blend_preds.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3835b3ee",
   "metadata": {},
   "source": [
    "## 15. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c894620",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL SUMMARY - HEART DISEASE PREDICTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nðŸ“Š PIPELINE:\")\n",
    "print(f\"  âœ“ Baseline Logistic Regression: {val_auc:.4f} AUC\")\n",
    "print(f\"  âœ“ LightGBM (5-fold): {lgb_mean:.4f} Â± {lgb_std:.4f}\")\n",
    "print(f\"  âœ“ XGBoost (5-fold): {xgb_mean:.4f} Â± {xgb_std:.4f}\")\n",
    "print(f\"  âœ“ CatBoost (5-fold): {cat_mean:.4f} Â± {cat_std:.4f}\")\n",
    "print(f\"  âœ“ Feature Engineering: {X_test_eng.shape[1]} features\")\n",
    "print(f\"  âœ“ Stacking Ensemble: 5-fold meta-features + Ridge\")\n",
    "print(f\"  âœ“ Blending: LGB 0.4 + XGB 0.4 + Stacking 0.2\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ SUBMISSION:\")\n",
    "print(f\"  Location: submissions/submission.csv\")\n",
    "print(f\"  Records: {len(submission_final)}\")\n",
    "print(f\"  Prediction range: [{blend_preds.min():.4f}, {blend_preds.max():.4f}]\")\n",
    "print(f\"  Status: âœ… Ready for upload\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
