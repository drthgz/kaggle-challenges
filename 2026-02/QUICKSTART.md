# 2026-02: Quick Setup & Execution Guide

## ðŸ“‹ Challenge Details
- **Competition**: Playground Series S6E2
- **Task**: Heart Disease Prediction (Binary Classification)
- **Metric**: AUC-ROC
- **Target**: Top 10 ranking
- **Duration**: Feb 1 - Feb 28, 2026

## ðŸš€ Quick Start

### Step 1: Download Data
```bash
cd /home/shiftmint/Documents/kaggle/kaggle-challenges/2026-02
# Download from Kaggle:
# - train.csv (put in data/)
# - test.csv (put in data/)
# - sample_submission.csv (put in data/)
```

### Step 2: Copy Required Files (if not auto-linked)
```bash
# Copy shared utilities if needed
cp ../shared/utils.py ./
```

### Step 3: Run Notebook
```bash
cd /home/shiftmint/Documents/kaggle/kaggle-challenges/2026-02
python -m jupyter notebook notebook.ipynb
# Or in VS Code: Open notebook.ipynb and run all cells
```

### Step 4: Upload Submission
```bash
# After notebook runs successfully:
# 1. Go to https://www.kaggle.com/competitions/playground-series-s6e2
# 2. Click "Make a submission"
# 3. Upload submissions/submission.csv
# 4. Check leaderboard ranking
```

## ðŸ“Š What the Notebook Does

1. **Loads data** - train.csv, test.csv, sample_submission.csv
2. **EDA** - Feature analysis, correlations with target
3. **Preprocessing** - Encode categorical, scale numeric features
4. **Baseline** - Logistic Regression for reference
5. **Model Comparison** - LightGBM vs XGBoost vs CatBoost (5-fold CV)
6. **Feature Engineering** - 6-8 new features (interactions, polynomials)
7. **Feature Validation** - Compare original vs engineered (3-fold CV)
8. **Stacking** - 5-fold meta-features + Ridge meta-learner
9. **Blending** - Weighted average (0.4 LGB, 0.4 XGB, 0.2 Stacking)
10. **Submission** - Generate predictions file

**Expected Runtime**: 3-5 minutes (optimized)

## ðŸŽ¯ Expected Performance

### Baseline
- Logistic Regression: ~0.80-0.85 AUC-ROC

### After Optimization
- LightGBM: 0.87-0.90
- XGBoost: 0.87-0.90
- Ensemble (Stacking + Blending): 0.88-0.92
- **Target: 0.89+ for top 10**

## ðŸ“ˆ Key Improvements from 2026-01

1. âœ… **Classification instead of regression**
   - Different model classes (Classifier vs Regressor)
   - Different output (probabilities vs continuous)
   - Different metric (AUC-ROC vs RMSE)

2. âœ… **Same ensemble architecture**
   - 5-fold stacking still effective
   - Blending with Ridge meta-learner works for both

3. âœ… **Same feature engineering**
   - Interactions, polynomials, ratios
   - Adapted for classification task

4. âœ… **Optimized hyperparameters**
   - Reduced tree counts (200-250 vs 400-500)
   - Smaller tree depths for classification
   - Conservative regularization

## ðŸ”§ File Structure

```
2026-02/
â”œâ”€â”€ notebook.ipynb              â† Main pipeline (30 cells)
â”œâ”€â”€ README.md                   (Challenge description)
â”œâ”€â”€ LEARNING_LOG.md             (Regression vs Classification guide)
â”œâ”€â”€ QUICKSTART.md               (This file)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.csv              (Download from Kaggle)
â”‚   â”œâ”€â”€ test.csv               (Download from Kaggle)
â”‚   â””â”€â”€ sample_submission.csv  (Download from Kaggle)
â””â”€â”€ submissions/
    â””â”€â”€ submission.csv         (Generated by notebook)
```

## ðŸ’¡ Lessons Applied from 2026-01

| Lesson | 2026-01 | 2026-02 |
|--------|---------|---------|
| Feature Engineering | +0.12% improvement | Same approach |
| Stacking | 5-fold meta-features | Same 5-fold |
| Blending | LGB 0.5, XGB 0.3, Stack 0.2 | LGB 0.4, XGB 0.4, Stack 0.2 |
| Runtime | 40% optimization | Applied here |
| CV Strategy | 5-fold KFold | 5-fold + Stratified split |

## ðŸŽ“ Key Differences from 2026-01

1. **Classification vs Regression**
   - Use LGBMClassifier not LGBMRegressor
   - Use `.predict_proba()[:, 1]` for probabilities
   - Evaluate with roc_auc_score not RMSE

2. **Different metric**
   - AUC-ROC (0.0-1.0, higher is better)
   - vs RMSE (continuous, lower is better)

3. **Different baseline**
   - LogisticRegression vs LinearRegression
   - Both work with same preprocessing

4. **Ensemble flexibility**
   - Ridge meta-learner works for both regression and classification!
   - Uses continuous outputs for metadata training

## âš¡ Quick Troubleshooting

**Data not found**:
- Make sure train.csv, test.csv, sample_submission.csv are in `data/` folder

**Import errors**:
- Ensure all packages installed: `pip install lightgbm xgboost catboost`
- Check Python 3.10+ with scikit-learn 1.1+

**Memory issues**:
- Reduce n_estimators in model builders
- Or use fewer CV folds (3 instead of 5)

**Submission format wrong**:
- Must have exactly 2 columns: `id, Heart Disease`
- Probabilities must be between 0.0 and 1.0
- Check submission_final.csv has correct format

## ðŸ“š Additional Resources

- **LEARNING_LOG.md** - Detailed regression vs classification guide
- **README.md** - Challenge overview
- **notebook.ipynb** - Complete implementation
- Previous: **2026-01/** - Reference for techniques

## ðŸŽ¯ Success Criteria

- âœ… Notebook runs without errors
- âœ… Submission file generated with correct format
- âœ… AUC-ROC > 0.88 (estimated)
- âœ… Leaderboard position < 24 (top 10%)
- âœ… Documentation complete

## ðŸš€ Next Steps After Submission

1. Check leaderboard position
2. If not top 10:
   - Apply StratifiedKFold systematically
   - Try Bayesian optimization for hyperparameters
   - Add neural network as base model
   - Calibrate probabilities
3. Document learnings for 2026-03

---

*Ready to compete! Follow the 4 steps above to get started.*
